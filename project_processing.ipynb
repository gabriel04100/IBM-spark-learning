{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark practise project Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in ./.venv/lib/python3.11/site-packages (3.2)\n",
      "Requirement already satisfied: pyspark in ./.venv/lib/python3.11/site-packages (3.5.2)\n",
      "Requirement already satisfied: findspark in ./.venv/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.venv/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "\n",
    "!pip install wget pyspark  findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the SparkContext.   \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 13:34:41 WARN Utils: Your hostname, gabriel-BOD-WXX9 resolves to a loopback address: 127.0.1.1; using 192.168.204.211 instead (on interface wlp0s20f3)\n",
      "24/09/23 13:34:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/23 13:34:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkContext object\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a Spark Session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv\" \n",
    "url2 = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data from external source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = wget.download(url1)\n",
    "dataset2 = wget.download(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.read.option(\"header\",\"true\").csv(\"./data/dataset1.csv\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"header\",\"true\").csv(\"./data/dataset2.csv\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, quarter, to_date,col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# casting to date format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----------+--------+\n",
      "|customer_id|date_column|amount|description|location|\n",
      "+-----------+-----------+------+-----------+--------+\n",
      "|          1| 2022-01-01|  5000| Purchase A| Store A|\n",
      "|          2| 2022-02-15|  1200| Purchase B| Store B|\n",
      "|          3| 2022-03-20|   800| Purchase C| Store C|\n",
      "+-----------+-----------+------+-----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)\n",
    "\n",
    "df1 =df1.withColumn(\"date_column\",to_date(col(\"date_column\"), \"d/M/yyyy\"))\n",
    "df2 =df2.withColumn(\"transaction_date\",to_date(col(\"transaction_date\"), \"d/M/yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adding year and quarter column to respectively df1 and df2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.withColumn(\"year\",year(col(\"date_column\")))\n",
    "df2=df2.withColumn(\"quarter\",quarter(col(\"transaction_date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**renaming columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.withColumnRenamed(\"amount\",\"transaction_amount\")\n",
    "df2 =df2.withColumnRenamed(\"value\",\"transaction_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+-----------+--------+----+\n",
      "|customer_id|date_column|transaction_amount|description|location|year|\n",
      "+-----------+-----------+------------------+-----------+--------+----+\n",
      "|          1| 2022-01-01|              5000| Purchase A| Store A|2022|\n",
      "|          2| 2022-02-15|              1200| Purchase B| Store B|2022|\n",
      "|          3| 2022-03-20|               800| Purchase C| Store C|2022|\n",
      "+-----------+-----------+------------------+-----------+--------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+----------------+-----------------+------+-------+\n",
      "|customer_id|transaction_date|transaction_value| notes|quarter|\n",
      "+-----------+----------------+-----------------+------+-------+\n",
      "|          1|      2022-01-01|             1500|Note 1|      1|\n",
      "|          2|      2022-02-15|             2000|Note 2|      1|\n",
      "|          3|      2022-03-20|             1000|Note 3|      1|\n",
      "+-----------+----------------+-----------------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**droping columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(\"description\",\"location\")\n",
    "df2 =df2.drop(\"notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|          1| 2022-01-01|              5000|2022|      2022-01-01|             1500|      1|\n",
      "|          2| 2022-02-15|              1200|2022|      2022-02-15|             2000|      1|\n",
      "|          3| 2022-03-20|               800|2022|      2022-03-20|             1000|      1|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = df1.join(df2, 'customer_id','inner')\n",
    "joined_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filtered df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|          1| 2022-01-01|              5000|2022|      2022-01-01|             1500|      1|\n",
      "|          2| 2022-02-15|              1200|2022|      2022-02-15|             2000|      1|\n",
      "|          4| 2022-04-10|              3000|2022|      2022-04-10|             2500|      2|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = joined_df.filter( joined_df[\"transaction_amount\"] >1000)\n",
    "filtered_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aggregate data by customer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|customer_id|sum(transaction_amount)|\n",
      "+-----------+-----------------------+\n",
      "|         51|                 4200.0|\n",
      "|         15|                 4200.0|\n",
      "|         54|                 1500.0|\n",
      "|         11|                 2200.0|\n",
      "|         69|                 5500.0|\n",
      "+-----------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_ammount_per_customer= filtered_df.groupBy('customer_id').agg(sum('transaction_amount'))\n",
    "total_ammount_per_customer.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**writte table to H table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ammount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ammount_per_customer.write.parquet(\"total_ammount_per_customer.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add new column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+\n",
      "|customer_id|date_column|transaction_amount|year|\n",
      "+-----------+-----------+------------------+----+\n",
      "|          1| 2022-01-01|              5000|2022|\n",
      "|          2| 2022-02-15|              1200|2022|\n",
      "|          3| 2022-03-20|               800|2022|\n",
      "+-----------+-----------+------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------+\n",
      "|customer_id|date_column|transaction_amount|year|high_value|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "|          1| 2022-01-01|              5000|2022|       Yes|\n",
      "|          2| 2022-02-15|              1200|2022|        No|\n",
      "|          3| 2022-03-20|               800|2022|        No|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumn(\"high_value\", when(col(\"transaction_amount\")  >= 5000,lit(\"Yes\")).otherwise(lit(\"No\")))\n",
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**calculate average by qurter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------------+-------+\n",
      "|customer_id|transaction_date|transaction_value|quarter|\n",
      "+-----------+----------------+-----------------+-------+\n",
      "|          1|      2022-01-01|             1500|      1|\n",
      "|          2|      2022-02-15|             2000|      1|\n",
      "|          3|      2022-03-20|             1000|      1|\n",
      "+-----------+----------------+-----------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "average_value_per_quarter=df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"average_trans_val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**total transaction value per year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------+\n",
      "|customer_id|date_column|transaction_amount|year|high_value|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "|          1| 2022-01-01|              5000|2022|       Yes|\n",
      "|          2| 2022-02-15|              1200|2022|        No|\n",
      "|          3| 2022-03-20|               800|2022|        No|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_per_year = df1.groupBy(\"year\").agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
